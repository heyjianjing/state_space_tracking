{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Estimator of parameters: frequentist"
      ],
      "metadata": {
        "id": "XCF-rYr4_Qw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Bias"
      ],
      "metadata": {
        "id": "9SdTlYLmaMR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Bias` of an estimator $\\hat{x}$ of a parameter $x$ (fixed, but unknown) is defined as\n",
        "\n",
        "$$B(\\hat{x})=\\mathbb{E}[\\hat{x}]-x$$\n",
        "\n",
        "The expected value is computed over distribution of data, an estimator is said to be unbiased when $B(\\hat{x})=0$"
      ],
      "metadata": {
        "id": "Uh1v9eD3aOCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Variance"
      ],
      "metadata": {
        "id": "ssLZW7JwavaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Variance` of an estimator $\\hat{x}$ of a parameter $x$ is defined as\n",
        "\n",
        "$$\\text{var}(\\hat{x})=\\sigma_{\\hat{x}}^2=\\mathbb{E}\\left[\\left(\\hat{x}-\\mathbb{E}[\\hat{x}]\\right)^2\\right]$$\n",
        "\n",
        "We would like variance to be as small as possible"
      ],
      "metadata": {
        "id": "d_M4x6fPaww0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Distribution and likelihood"
      ],
      "metadata": {
        "id": "EhUtuLaDDtCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have a system with input $x$ and output $y$, and\n",
        "\n",
        "$$y=x+v, v\\sim N(0, \\sigma^2)$$\n",
        "\n",
        "Suppose we obtain a `single` observation $y$, we can write\n",
        "\n",
        "$$p(y|x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left[-\\frac{1}{2\\sigma^2}(y-x)^2\\right]$$\n",
        "\n",
        "Here, $x$ is the `parameter` we want to estimate and $y$ is observed value, or `data`"
      ],
      "metadata": {
        "id": "hE8gyFYMDwaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When $x$ is given, this is a probability `distribution` (i.e., PDF) of $y$ parameterized by $x$\n",
        "\n",
        "However, when $y$ is given, this is a `likelihood function` of parameter $x$ (which is no longer a distribution and does not have to integrate to one over $x$)"
      ],
      "metadata": {
        "id": "6an_8P1pEyAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The $\\hat{x}$ obtained by maximizing this likelihood function is known as `maximum likelihood` estimator (MLE)\n",
        "\n",
        "From the expression, it is not difficult to see that it (also happens to) minimize the mean squared error (MSE) for Gaussian distribution\n",
        "\n",
        "$$\\hat{x}_{\\text{MLE}}=\\arg \\max_{x}p(y|x) = \\arg \\min_x (y-x)^2= y$$\n",
        "\n",
        "Here, the MSE refers to `empirical (sum of) squared residual(s)` bewteen data and the estimator"
      ],
      "metadata": {
        "id": "kFMtv8M8Fjmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we consider $y$ as a random variable (due to noise $v$), we can sample many of them $y_{1:N}$, in this case\n",
        "\n",
        "$$\\begin{align*}\\hat{x}_{\\text{MLE}}&=\\arg \\max_x p(y_1, \\cdots, y_N|x)\\\\\n",
        "&=\\arg \\max_x \\prod_{i=1}^N p(y_i|x) \\\\\n",
        "&=\\arg \\max_x \\sum_{i=1}^N \\log p(y_i|x) \\\\\n",
        "&=\\arg \\min_x \\sum_{i=1}^N (y_i-x)^2 \\\\\n",
        "& \\text{take derivative w.r.t. x and set to zero} \\\\\n",
        "&=\\frac{1}{N}\\sum_{i=1}^Ny_i\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "-wlzZmUUkaUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### MSE of estimator"
      ],
      "metadata": {
        "id": "SSPYgrDgm34T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider $y$ as distribution, `MSE of the estimator` is defined as\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{MSE}&=\\mathbb{E}_{y|x}\\left[\\left(\\hat{x}(y)-x\\right)^2\\right]\\\\\n",
        "&=\\int \\left(\\hat{x}(y)-x\\right)^2p(y|x)dy\n",
        "\\end{align*}$$\n",
        "\n",
        "That is, the `expectation of squared difference` between estimator and true $x$ considering all possible $y$ generated by the distribution parameterized by this $x$\n",
        "\n",
        "We can evalute this by sampling\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{MSE}&=\\frac{1}{N}\\sum_{i=1}^N\\left(\\hat{x}(y_i)-x\\right)^2\n",
        "\\end{align*}$$\n",
        "\n",
        "It can be shown that the MLE also minimizes this MSE among all unbiased estimators for Gaussian problem"
      ],
      "metadata": {
        "id": "RpHg3_2Hm6IF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Estimator of parameters: Bayesian"
      ],
      "metadata": {
        "id": "Bvte5LDdtdci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bayesian approach considers $x$ a random variable, rather than a fixed but unknown value, with its own distribution\n",
        "\n",
        "This means in MSE, the expected value will be evaluated over the joint PDF $p(x,y)$\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{MSE}(\\hat{x})&=\\iint \\left(\\hat{x}-x\\right)^2p(x, y)dxdy \\\\\n",
        "& \\text{split using } p(x, y)=p(x|y)p(y)\\\\\n",
        "&=\\int \\left[\\int\\left(\\hat{x}-x\\right)^2p(x|y)dx \\right]p(y)dy\n",
        "\\end{align*}$$\n",
        "\n",
        "Since $p(y)\\geq 0$, we can minimize the term in the bracket for each $y$, then the Bayesian MSE will be minimized"
      ],
      "metadata": {
        "id": "YY-ibwfOtj02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take derivative w.r.t. $\\hat{x}$\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\frac{\\partial}{\\partial \\hat{x}}\\text{MSE} &\\propto\\frac{\\partial}{\\partial \\hat {x}}\\int (x-\\hat{x})^2p(x|y)dx \\\\\n",
        "&=\\int\\frac{\\partial}{\\partial \\hat {x}} (x-\\hat{x})^2p(x|y)dx \\\\\n",
        "&=-2\\int(x-\\hat{x})p(x|y)dx \\\\\n",
        "&=-2 \\int xp(x|y)dx +2\\hat{x}\\int p(x|y) dx \\\\\n",
        "&=-2 \\int xp(x|y)dx + 2\\hat{x}\n",
        "\\end{align*}$$\n",
        "\n",
        "Set it to zero and we have\n",
        "\n",
        "$$\\hat{x}=\\int x p(x|y)dx=\\mathbb{E}[x|y]$$\n",
        "\n",
        "We see that the estimator minimizing the Bayesian MSE is the mean of the `posterior` PDF $\\mathbb{E}[x|y]$, after data has been observed"
      ],
      "metadata": {
        "id": "mZztdQHEuiUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### State space tracking"
      ],
      "metadata": {
        "id": "6jv35uXRZXSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a system with state $x$ and observation $y$, we define `state space` model $f_n$ and `measurement` model $h_n$ as\n",
        "\n",
        "$$\\begin{align*}x_{n+1}&=f_n(x_n, u_n)\\\\\n",
        "y_n&=h_n(x_n, v_n)\n",
        "\\end{align*}$$\n",
        "\n",
        "Primary goal: estimate state of system $x_n$ at some time step $n$, given a sequence of observations $y_{0:n}$, with $u_n, v_n$ being noise terms\n",
        "\n",
        "Often, compact notation $\\hat{x}_{n|n+l}$ is used, denoting estimate of state at step $n$, based on $y_{0:n+l}$ observations\n",
        "\n",
        "* when $l=-1$: `predicted` estimate\n",
        "* when $l=0$: `filtered` estimate (our focus)\n",
        "\n",
        "We want to do it `recursively`, so we don't have to retain all past measurements"
      ],
      "metadata": {
        "id": "IbAJK0-eZZIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian estimation is all about finding the `(marginal) posterior` $p(x_n|y_{0:n})$ on top of which we can compute any metric we want, mean, median, whatever\n",
        "\n",
        "We assume `Markov` property for state space model, that is, if we know what state is at step $n$, then there is no additional information gained from knowing previous values of the state\n",
        "\n",
        "$$p(x_{n+1}|x_{0:n})=p(x_{n+1}|x_n)$$\n",
        "\n",
        "Also for measurement model\n",
        "\n",
        "$$p(y_n|x_{0:n})= p(y_n|x_{n})$$\n",
        "\n",
        "However\n",
        "\n",
        "$$p(x_n|y_{0:n})\\neq p(x_n|y_{n})$$"
      ],
      "metadata": {
        "id": "iLIk_tpiBfs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bayesian joint PDF recursions"
      ],
      "metadata": {
        "id": "ygnGCJWTFpLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to find a recursive equation from $p(x_{0:n-1}|y_{0:n-1})$ to $p(x_{0:n}|y_{0:n})$\n",
        "\n",
        "First, we split the following\n",
        "\n",
        "$$\\begin{align*}p(x_{0:n}, y_n|y_{0:n-1})&=p(x_{0:n}|y_n,y_{0:n-1})p(y_n|y_{0:n-1})\\\\\n",
        "&=p(x_{0:n}|y_{0:n})p(y_n|y_{0:n-1})\n",
        "\\end{align*}$$\n",
        "\n",
        "and, we can also do split the other way\n",
        "\n",
        "$$\\begin{align*}p(x_{0:n}, y_n|y_{0:n-1})&=p(y_n|y_{0:n-1}, x_{0:n})p(x_{0:n}|y_{0:n-1}) \\\\\n",
        "& \\text{Markov property} \\\\\n",
        "&=p(y_n|x_n)p(x_{0:n}|y_{0:n-1})\\\\\n",
        "&=p(y_n|x_n)p(x_n, x_{0:n-1}|y_{0:n-1})\\\\\n",
        "& \\text{split the second term} \\\\\n",
        "&=p(y_n|x_n)p(x_n|x_{0:n-1}, y_{0:n-1})p(x_{0:n-1}|y_{0:n-1})\\\\\n",
        "& \\text{Markov property} \\\\\n",
        "&=p(y_n|x_n)p(x_n|x_{n-1})p(x_{0:n-1}|y_{0:n-1})\\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "Combine two results, we have our recursive equation for joint posterior PDF\n",
        "\n",
        "$$p(x_{0:n}|y_{0:n})=p(x_{0:n-1}|y_{0:n-1})\\frac{p(y_n|x_n)p(x_n|x_{n-1})}{p(y_n|y_{0:n-1})}$$\n",
        "\n",
        "with likelihood $p(y_n|x_n)$ and prior $p(x_n|x_{n-1})$ in the numerator, and the denominator is a normalizing constant not related to the state"
      ],
      "metadata": {
        "id": "HWsgng5dFr9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In theory, we can obtain the marginal through integration\n",
        "\n",
        "$$p(x_n|y_{0:n})=\\int p(x_{0:n}|y_{0:n})dx_{0:n-1}$$\n",
        "\n",
        "But it beats the purpose of not having to computing the joint PDF in the first place"
      ],
      "metadata": {
        "id": "4B3lSJk5LZiK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcHfOapMZYrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}