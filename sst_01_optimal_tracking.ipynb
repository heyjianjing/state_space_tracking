{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Motivation"
      ],
      "metadata": {
        "id": "6jv35uXRZXSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a system with state $x$ and observation $y$, we define state space model $f_n$ and observation model $h_n$ as\n",
        "\n",
        "$$\\begin{align*}x_{n+1}&=f_n(x_n, u_n)\\\\\n",
        "y_n&=h_n(x_n, v_n)\n",
        "\\end{align*}$$\n",
        "\n",
        "Primary goal: estimate state of system $x_n$ at some time step $n$, given a sequence of observations $y_{1:n}$, with $u_n, v_n$ being noise terms\n",
        "\n",
        "So, what is a `good` estimator?"
      ],
      "metadata": {
        "id": "IbAJK0-eZZIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Frequentist"
      ],
      "metadata": {
        "id": "7etZ33V2gzFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Bias"
      ],
      "metadata": {
        "id": "9SdTlYLmaMR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Bias` of an estimator $\\hat{x}$ of a parameter $x$ (fixed, but unknown) is defined as\n",
        "\n",
        "$$B(\\hat{x})=\\mathbb{E}[\\hat{x}]-x$$\n",
        "\n",
        "The expected value is computed over distribution of data, an estimator is said to be unbiased when $B(\\hat{x})=0$"
      ],
      "metadata": {
        "id": "Uh1v9eD3aOCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Variance"
      ],
      "metadata": {
        "id": "ssLZW7JwavaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Variance` of an estimator $\\hat{x}$ of a parameter $x$ is defined as\n",
        "\n",
        "$$\\text{var}(\\hat{x})=\\sigma_{\\hat{x}}^2=\\mathbb{E}\\left[\\left(\\hat{x}-\\mathbb{E}[\\hat{x}]\\right)^2\\right]$$\n",
        "\n",
        "We would like variance to be as small as possible"
      ],
      "metadata": {
        "id": "d_M4x6fPaww0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Distribution and likelihood"
      ],
      "metadata": {
        "id": "EhUtuLaDDtCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have a system with input $x$ and output $y$, and\n",
        "\n",
        "$$y=x+v, v\\sim N(0, \\sigma^2)$$\n",
        "\n",
        "Suppose we obtain a `single` observation $y$, we can write\n",
        "\n",
        "$$p(y|x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left[-\\frac{1}{2\\sigma^2}(y-x)^2\\right]$$\n",
        "\n",
        "Here, $x$ is the `parameter` we want to estimate and $y$ is observed value, or `data`"
      ],
      "metadata": {
        "id": "hE8gyFYMDwaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When $x$ is given, this is a probability `distribution` (i.e., PDF) of $y$ parameterized by $x$\n",
        "\n",
        "However, when $y$ is given, this is a `likelihood function` of parameter $x$ (which is no longer a distribution and does not have to integrate to one over $x$)"
      ],
      "metadata": {
        "id": "6an_8P1pEyAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The $\\hat{x}$ obtained by maximizing this likelihood function is known as `maximum likelihood` estimator (MLE)\n",
        "\n",
        "From the expression, it is not difficult to see that it (also happens to) minimize the mean squared error (MSE) for Gaussian distribution\n",
        "\n",
        "$$\\hat{x}_{\\text{MLE}}=\\arg \\max_{x}p(y|x) = \\arg \\min_x (y-x)^2= y$$\n",
        "\n",
        "Here, the MSE refers to `empirical sum of squared residuals` bewteen data and the estimator"
      ],
      "metadata": {
        "id": "kFMtv8M8Fjmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we consider $y$ as a random variable (due to noise $v$), we can sample many of them $y_{1:N}$, in this case\n",
        "\n",
        "$$\\begin{align*}\\hat{x}_{\\text{MLE}}&=\\arg \\max_x p(y_1, \\cdots, y_N|x)\\\\\n",
        "&=\\arg \\max_x \\prod_{i=1}^N p(y_i|x) \\\\\n",
        "&=\\arg \\max_x \\sum_{i=1}^N \\log p(y_i|x) \\\\\n",
        "&=\\arg \\min_x \\sum_{i=1}^N (y_i-x)^2 \\\\\n",
        "&=\\frac{1}{N}\\sum_{i=1}^Ny_i\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "-wlzZmUUkaUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### MSE of estimator"
      ],
      "metadata": {
        "id": "SSPYgrDgm34T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider $y$ as distribution, `MSE of the estimator` is defined as\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{MSE}&=\\mathbb{E}_{y|x}\\left[\\left(\\hat{x}(y)-x\\right)^2\\right]\\\\\n",
        "&=\\int \\left(\\hat{x}(y)-x\\right)^2p(y|x)dy\n",
        "\\end{align*}$$\n",
        "\n",
        "That is, the `expected squared difference` between estimator and true $x$ considering all possible $y$ generated by the distribution parameterized by this $x$\n",
        "\n",
        "We can also evalute this using Monte Carlo approach by sampling\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{MSE}&=\\frac{1}{N}\\sum_{i=1}^N\\left(\\hat{x}(y_i)-x\\right)^2\n",
        "\\end{align*}$$\n",
        "\n",
        "It can be shown that the MLE also minimizes this MSE among all unbiased estimators for Gaussian problem"
      ],
      "metadata": {
        "id": "RpHg3_2Hm6IF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bayesian"
      ],
      "metadata": {
        "id": "Bvte5LDdtdci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bayesian approach considers $x$ also a random variable, rather than a fixed but unknown value, with its own distribution\n",
        "\n",
        "This means in MSE, the expected value will be evaluated over the joint PDF\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{MSE}(\\hat{x})&=\\iint \\left(\\hat{x}-x\\right)^2p(x, y)dxdy \\\\\n",
        "& \\text{Bayes' rule} \\\\\n",
        "&=\\int \\left[\\int\\left(\\hat{x}-x\\right)^2p(x|y)dx \\right]p(y)dy\n",
        "\\end{align*}$$\n",
        "\n",
        "Since $p(y)\\geq 0$, we can minimize the term in the bracket for each y, then the Bayesian MSE will be minimized"
      ],
      "metadata": {
        "id": "YY-ibwfOtj02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take derivative w.r.t. $\\hat{x}$\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\frac{\\partial}{\\partial \\hat{x}}\\text{MSE} &\\propto\\frac{\\partial}{\\partial \\hat {x}}\\int (x-\\hat{x})^2p(x|y)dx \\\\\n",
        "&=\\int\\frac{\\partial}{\\partial \\hat {x}} (x-\\hat{x})^2p(x|y)dx \\\\\n",
        "&=-2\\int(x-\\hat{x})p(x|y)dx \\\\\n",
        "&=-2 \\int xp(x|y)dx +2\\hat{x}\\int p(x|y) dx \\\\\n",
        "&=-2 \\int xp(x|y)dx + 2\\hat{x}\n",
        "\\end{align*}$$\n",
        "\n",
        "Set it to zero and we have\n",
        "\n",
        "$$\\hat{x}=\\int x p(x|y)dx=\\mathbb{E}[x|y]$$\n",
        "\n",
        "or, the estimator that minimizes the Bayesian MSE is the mean of the `posterior` PDF $\\mathbb{E}[x|y]$, which is after data has been observed"
      ],
      "metadata": {
        "id": "mZztdQHEuiUR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcHfOapMZYrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
