{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "RXTo9PJLGaxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `changepoint` is an underlying shift in the parameters that generate a data sequence (e.g. the mean of a Gaussian suddenly jumps). Here, we focus on the online task\n",
        "\n",
        "To do this, after getting data point $x_t$, we track a latent run-length $r_t$ that counts how many data points have been generated by the current set of parameters (current run)\n",
        "\n",
        "  - If no changepoint occurs, the run length grows by one from $r_{t-1}$\n",
        "  - If a changepoint occurs, the old run's final length was $r_{t-1}+1$; we reset $r_t$ to `zero` for the brand-new current run\n",
        "\n",
        "We want to maintains full run length `posterior` $p(r_t|x_{1:t})$ and update this `recursively`\n",
        "\n",
        "In addition, we want to update the sequence `predictive distribution` for $x_{t+1}$ using only $x_{1:t}$\n"
      ],
      "metadata": {
        "id": "RyYZGXpDFiF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predictive distribution"
      ],
      "metadata": {
        "id": "ym9Sr6iSG_Ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using marginalization, we can write the `predictive` as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x_{t+1}|x_{1:t})&=\\sum_{r_t=0}^tp(x_{t+1}|r_t, x_{t-r_t:t})p(r_t|x_{1:t}) \\\\\n",
        "&=\\sum_{r_t=0}^tp(x_{t+1}|r_t, x_t^{(r)})p(r_t|x_{1:t})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $x_t^{(r)}$ denotes the portion of $x_{1:t}$ that belongs to the `current` run\n",
        "\n",
        "This shows that `sequence predictive` is determined by $p(x_{t+1}|r_t, x_t^{(r)})$ and `run length posterior` $p(r_t|x_{1:t})$\n",
        "\n",
        "$p(x_{t+1}|r_t, x_t^{(r)})$ is often refered to as underlying probabilistic model `(UPM) predictive` to distinguish it from sequence predictive $p(x_{t+1}|x_{1:t})$"
      ],
      "metadata": {
        "id": "vN9ST-GGHCIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we look at run length posterior and UPM predictive"
      ],
      "metadata": {
        "id": "Vr0Uq-yo9Jdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run length posterior"
      ],
      "metadata": {
        "id": "zpxNKzVqJxxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute run length posterior, we use the expression of conditional probability\n",
        "\n",
        "$$\\begin{align*}\n",
        "p(r_t|x_{1:t})&=\\frac{p(r_t, x_{1:t})}{\\sum_{r_{t'}}p(r_{t'},x_{1:t})}\n",
        "\\end{align*}$$\n",
        "\n",
        "We express the joint in a `recursive` manner\n",
        "\n",
        "$$\\begin{align*}\n",
        "p(r_t, x_{1:t})&=\\sum_{r_{t-1}}p(r_t, r_{t-1}, x_t, x_{1:t-1}) \\\\\n",
        "&=\\sum_{r_{t-1}}p(r_t, x_t | r_{t-1},  x_{1:t-1})p(r_{t-1}, x_{1:t-1})\\\\\n",
        "&=\\sum_{r_{t-1}}p(x_t|r_t, r_{t-1}, x_{1:t-1})p(r_t|r_{t-1}, x_{1:t-1})p(r_{t-1}, x_{1:t-1}) \\\\\n",
        "& \\text{assumption: } x_t \\text{ conditionally independent of } r_t \\\\\n",
        "& \\text{assumption: } r_t \\text{ conditionally independent of } x_{1:t-1} \\\\\n",
        "&=\\sum_{r_{t-1}}p(x_t|r_{t-1}, x_{1:t-1})p(r_t|r_{t-1})p(r_{t-1}, x_{1:t-1})\\\\\n",
        "&=\\sum_{r_{t-1}}p(x_t|r_{t-1}, x_{t-1}^{(r)})p(r_t|r_{t-1})p(r_{t-1}, x_{1:t-1})\\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "Therefore, the joint at step $t$, $p(r_t, x_{1:t})$, depends on UPM predictive $p(x_t|r_{t-1}, x_{t-1}^{(r)})$ and joint $p(r_{t-1}, x_{1:t-1})$ at step $t-1$ plus a changepoint prior $p(r_t|r_{t-1})$"
      ],
      "metadata": {
        "id": "6_cUd86zJ1o1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is noted from derivation above that once we set the initial joint $p(r_0)$, what remains to do is to efficiently update UPM predictive and compute changepoint prior"
      ],
      "metadata": {
        "id": "wdtilTHGRt1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conjugacy"
      ],
      "metadata": {
        "id": "e0w_Ly1jS2TZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The computation of UPM predictive leverages `conjugate model`"
      ],
      "metadata": {
        "id": "qboWzVjm9j8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have observations $D$, model parameters $\\eta$ and hyperparameters $\\nu$. Then the prior predictive distribution marginalized over parameters can be written as\n",
        "\n",
        "$$p(x|\\nu)=\\int p(x|\\eta)p(\\eta |\\nu)d\\eta$$\n",
        "\n",
        "where $p(x|\\eta)$ is `predictive model` given parameters and $(\\eta |\\nu)$ is the prior of `parameters`\n",
        "\n",
        "This is called `prior predictive distribution` because this is the prediction `before` we observe any data (that is, $D$ is not taken into account)"
      ],
      "metadata": {
        "id": "1s1ErMYWS3tW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can write `posterior predictive distribution` as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x|D, \\nu)&=\\int p(x|\\eta)p(\\eta |D, \\nu)d\\eta \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "A wonderful property of `conjugate` model is that the prior distribution and posterior distribution are of the same form. Therefore, if the prior and posterior of parameter are conjugate w.r.t. certain likelihood function, then we have\n",
        "\n",
        "$$p(\\eta|D, \\nu) = p(\\eta | \\nu')$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x|D, \\nu)&=\\int p(x|\\eta)p(\\eta |D, \\nu)d\\eta \\\\\n",
        "&=\\int p(x|\\eta)p(\\eta |\\nu')d\\eta\\\\\n",
        "&=p(x|\\nu')\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "That is, the posterior predictive distribution is in the `same form` as the prior predictive with only changed hyperparameters $\\nu'$\n",
        "\n",
        "This allows us to bypass the whole integration thing provided that we can compute $\\nu'$"
      ],
      "metadata": {
        "id": "AmSgonGpUI3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example of Gaussian"
      ],
      "metadata": {
        "id": "ocUJvY3L-Lwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, consider a Gaussian distribution with parameter $\\mu$ and `fixed` variance $\\sigma^2$, where $\\mu$ is determined by two hyperparameters $\\mu_0$ and $\\sigma_0^2$, or with `prior` $\\mu \\sim N(\\mu_0, \\sigma_0^2)$\n",
        "\n",
        "If we have $n$ data points $x_{1:n}$, we can write out the `likelihood`\n",
        "\n",
        "$$\n",
        "\\begin{align*}L(x_{1:n}|\\mu, \\sigma^2)&=\\prod_{i=1}^n p(x_i|\\mu) \\\\\n",
        "&=\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right]\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "MeeDaKjBFwJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now write the `joint`\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x_{1:n}, \\mu) &= \\frac{1}{\\sqrt{2\\pi \\sigma_0^2}}\\exp\\left[-\\frac{1}{2\\sigma_0^2}(\\mu-\\mu_0)^2\\right]\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right]\\\\\n",
        "&\\propto \\exp\\left[-\\frac{1}{2\\sigma_0^2}\\left(\\mu^2-2\\mu \\mu_0+\\mu_0^2\\right)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\\left(x_i^2-2x_i\\mu+\\mu^2\\right)\\right]\\\\\n",
        "&=\\exp \\left[-\\frac{\\mu^2}{2\\sigma_0^2}+\\frac{\\mu\\mu_0}{\\sigma_0^2}-\\frac{\\mu_0^2}{2\\sigma_0^2}-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2 +\\frac{\\mu}{\\sigma^2}\\sum_{i=1}^nx_i -\\frac{\\mu^2}{2\\sigma^2}n\\right] \\\\\n",
        "&\\propto \\exp\\left[-\\frac{\\mu^2\\sigma^2+\\mu^2\\sigma_0^2n}{2\\sigma_0^2\\sigma^2}+\\frac{\\mu\\mu_0\\sigma^2+\\mu\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma_0^2\\sigma^2}\\right] \\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left((\\sigma^2+\\sigma_0^2n)\\mu^2-2(\\mu_0\\sigma^2+\\sigma_0^2\\sum_{i=1}^nx_i)\\mu\\right)\\right]\\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left(a\\mu^2+b\\mu+c\\right)\\right] \\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left(a(\\mu-d)^2+e\\right)\\right]\\\\\n",
        "a&=\\sigma^2+\\sigma_0^2n \\\\\n",
        "d&=-\\frac{b}{2a}=\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\\\\n",
        "e&=c-\\frac{b^2}{4a}=-\\frac{(\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i)^2}{\\sigma^2+\\sigma_0^2n} \\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left((\\sigma^2+\\sigma_0^2n)\\left(\\mu-\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\right)^2-\\frac{(\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i)^2}{\\sigma^2+\\sigma_0^2n}\\right)\\right] \\\\\n",
        "&\\propto \\exp\\left[-\\frac{\\sigma^2+\\sigma_0^2n}{2\\sigma_0^2\\sigma^2}\\left(\\mu-\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\right)^2\\right]\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "MXqoLiAMMcAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Bayes' rule, we know that `posterior` is proportional to this joint and therefore\n",
        "\n",
        "$$p(\\mu|x_{1:n})\\propto \\exp\\left[-\\frac{\\sigma^2+\\sigma_0^2n}{2\\sigma_0^2\\sigma^2}\\left(\\mu-\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\right)^2\\right]$$\n",
        "\n",
        "which is a Gaussian distribution as well"
      ],
      "metadata": {
        "id": "r19srax5RWtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The posterior `variance` is\n",
        "\n",
        "$$\\sigma_n^2 = \\left[\\frac{\\sigma_0^2\\sigma^2}{\\sigma^2+\\sigma_0^2}\\right]^2$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{1}{\\sigma_n^2} &=\\frac{\\sigma^2+\\sigma_0^2n}{\\sigma_0^2\\sigma^2} \\\\\n",
        "&=\\frac{\\sigma^2}{\\sigma_0^2\\sigma^2}+\\frac{\\sigma_0^2}{\\sigma^2\\sigma_0^2}n \\\\\n",
        "&=\\frac{1}{\\sigma_0^2}+\\frac{1}{\\sigma^2}n\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\sigma_n^2 =\\frac{\\sigma_0^2\\sigma^2}{\\sigma^2+\\sigma_0^2n}= \\frac{1}{\\frac{1}{\\sigma_0^2}+\\frac{1}{\\sigma^2}n}$$"
      ],
      "metadata": {
        "id": "eYb3Mv9UngGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The posterior `mean` is\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mu_n &= \\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n} \\\\\n",
        "&=\\frac{\\sigma^2}{\\sigma^2+\\sigma_0^2n}\\mu_0+\\frac{\\sigma_0^2}{\\sigma^2+\\sigma_0^2n}\\sum_{i=1}^nx_i \\\\\n",
        "&=\\sigma_n^2\\left(\\frac{\\mu_0}{\\sigma_0^2}+\\frac{1}{\\sigma^2}\\sum_{i=1}^nx_i\\right)\n",
        "\\end{align*}$$\n",
        "\n",
        "Recall that $\\frac{1}{n}\\sum_{i=1}^nx_i$ is the `maximum likelihood` estimate of $\\mu$\n",
        "\n",
        "So the posterior mean is a weighted sum of prior mean $\\mu_0$ and the ML estimate $\\mu_{ML}$\n"
      ],
      "metadata": {
        "id": "bt1J4AK_nijW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Derivation from perspective of optimization"
      ],
      "metadata": {
        "id": "Z7OV-olZsYSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the posterior $p(\\mu| x_{1:n})$ is Gaussian, therefore, the center of posterior density is also the maximum-a-posteriori (MAP) estimate of the mean $\\mu_n=\\mu_{\\text{MAP}}$\n",
        "\n",
        "If we just want to get this MAP estimate, we can take derivative of $\\log p(\\mu|x_{1:n})$ or for calculation, the log of joint $\\log p(x_{1:n}, \\mu)$ (since the difference in constant vanishes after differentiation)\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{d}{d\\mu}\\log p(x_{1:n}, \\mu)&=\\frac{d}{d\\mu}\\left[-\\frac{1}{2\\sigma_0^2}(\\mu-\\mu_0)^2-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2+\\text{const.}\\right] \\\\\n",
        "&=\\frac{d}{d\\mu}\\left[-\\frac{1}{2\\sigma_0^2}(\\mu^2-2\\mu\\mu_0)-\\frac{1}{2\\sigma^2}\\left(n\\mu^2-2\\mu\\sum_{i=1}^nx_i\\right)+\\text{const.}\\right] \\\\\n",
        "& \\text{group } \\mu^2 \\text{ and } \\mu \\\\\n",
        "&=\\frac{d}{d\\mu}\\left[-\\frac{\\sigma^2+n\\sigma_0^2}{2\\sigma_0^2\\sigma^2}\\mu^2+\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma_0^2\\sigma^2}\\mu +\\text{const.}\\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "and set to zero, we have\n",
        "\n",
        "$$\\frac{\\sigma^2+n\\sigma_0^2}{\\sigma_0^2\\sigma^2}\\mu=\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma_0^2\\sigma^2} $$\n",
        "\n",
        "Solve for the MAP estimate\n",
        "\n",
        "$$\\mu_{\\text{MAP}}=\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+n\\sigma_0^2}$$\n",
        "\n",
        "which is the same as the posterior mean $\\mu_n$ we get earlier\n",
        "\n",
        "Both approaches, taking the derivative or completing the square, are mathematically identical\n",
        "\n",
        "The latter just skips writing the derivative because the result can be read off immediately once the quadratic is centered"
      ],
      "metadata": {
        "id": "qKofWDrdsao5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same can be done for posterior variance\n",
        "\n",
        "Since the posterior is Gaussian, we know\n",
        "\n",
        "$$\\log p(\\mu|x_{1:n})=-\\frac{1}{2\\sigma_n^2}(\\mu-\\mu_n)^2+\\text{const.}$$\n",
        "\n",
        "Take second derivative w.r.t. $\\mu$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\left.\\frac{d^2}{d\\mu^2}\\log p(\\mu|x_{1:n})\\right|_{\\mu=\\mu_n}\n",
        "&=\\left.\\frac{d^2}{d\\mu^2}\\log p(\\mu,x_{1:n})\\right|_{\\mu=\\mu_n}\\\\\n",
        "&=-\\frac{1}{\\sigma_n^2} \\\\\n",
        "&=-\\frac{\\sigma^2+n\\sigma_0^2}{\\sigma_0^2\\sigma^2}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "and we get posterior variance as before\n",
        "\n",
        "$$\\sigma_n^2 =\\frac{\\sigma_0^2\\sigma^2}{\\sigma^2+n\\sigma_0^2}$$"
      ],
      "metadata": {
        "id": "3PYvFqKOyCZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exponential family"
      ],
      "metadata": {
        "id": "x_P2FDM1YAhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More generally, it is the exponential family that is of interest\n",
        "\n",
        "Standard distributions can be cast into a canonical form (we can think of it as `likelihood`)\n",
        "\n",
        "$$p(x|\\eta)=h(x)g(\\eta)\\exp \\left[\\eta^T u(x)\\right]$$\n",
        "\n",
        "where\n",
        "  - $h(x)$ is `underlying measure` carrying every factor that does not involve $\\eta$\n",
        "  - $u(x)$ is sufficient-statistic of `data`\n",
        "  - $\\eta$ is `natural-parameter`\n",
        "  - $g(\\eta)$ is normalizer\n",
        "\n",
        "$$\\begin{align*}\n",
        "g(\\eta) \\int h(x) \\exp\\left[\\eta^T u(x)\\right]dx=1\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "6q_kqmn1bWqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Sufficient statistic"
      ],
      "metadata": {
        "id": "-IjrVAHn1iXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For data $x_{1:n}$, the idea of sufficient statistic is that we can use\n",
        "\n",
        "$$\\sum_{i=1}^n u(x_{i})$$\n",
        "\n",
        "to compute maximum likelihood estimate of natural parameters\n",
        "\n",
        "Use canonical form, we can write the log-likelihood as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\log p(x_{1:n}|\\eta)&=\\log \\prod_{i=1}^n p(x_i|\\eta)\\\\\n",
        "&=\\log \\left[\\left(\\prod_{i=1}^nh(x_i)\\right)g(\\eta)^n\\exp\\left(\\eta^T\\sum_{i=1}^nu(x_i)\\right)\\right]\\\\\n",
        "&=\\log \\left(\\prod_{i=1}^nh(x_i)\\right)+n\\log g(\\eta)+\\eta^T\\sum_{i=1}^nu(x_i)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Take gradient w.r.t. $\\eta$ and set to zero\n",
        "\n",
        "$$\\nabla n\\log g(\\eta_{\\text{ML}})+\\sum_{i=1}^nu(x_i)=0$$\n",
        "\n",
        "or\n",
        "\n",
        "$$-\\nabla \\log g(\\eta_{\\text{ML}}) = \\frac{1}{n}\\sum_{i=1}^nu(x_i)$$\n",
        "\n",
        "Note that since sufficient statistic is a running sum, we can compute it `incrementally` as new data arrives"
      ],
      "metadata": {
        "id": "En2w47YY1ldG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Moments through differentiation"
      ],
      "metadata": {
        "id": "N29aN44R-GQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Denote log normalizer\n",
        "\n",
        "$$A(\\eta)=-\\log g(\\eta)=\\log \\int h(x)\\exp \\left[\\eta^Tu(x)\\right]dx$$\n",
        "\n",
        "Take gradient w.r.t. $\\eta$\n",
        "\n",
        "$$\\nabla A(\\eta)=\\frac{1}{Z(\\eta)}\\nabla Z(\\eta)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$Z(\\eta)=\\exp[A(\\eta)]=\\int h(x) \\exp\\left[\\eta^Tu(x)\\right]dx$$\n",
        "\n",
        "Under some mild condition, we can write\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla Z(\\eta)&=\\int h(x)u(x)\\exp\\left[\\eta^Tu(x)\\right]dx \\\\\n",
        "&=\\int u(x)Z(\\eta)\\frac{h(x)\\exp\\left[\\eta^Tu(x)\\right]}{Z(\\eta)}dx\\\\\n",
        "& \\text{the fraction is a density on }x\\\\\n",
        "&=\\mathbb{E}\\left[Z(\\eta)u(x)\\right]\\\\\n",
        "&=Z(\\eta)\\mathbb{E}\\left[u(x)\\right]\n",
        "\\end{align*}$$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$$\\nabla A(\\eta)=\\frac{1}{Z(\\eta)}\\nabla Z(\\eta)=\\mathbb{E}\\left[u(x)\\right]$$"
      ],
      "metadata": {
        "id": "sG4XvVXW-N1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Conjugate prior"
      ],
      "metadata": {
        "id": "6-SKwoEl-6Kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For exponential family member, it has a conjugate `prior` of form\n",
        "\n",
        "$$p(\\eta|\\chi, \\nu)=f(\\chi, \\nu)g(\\eta)^{\\nu}\\exp\\left[\\eta^T\\chi\\right]$$\n",
        "\n",
        "where $\\nu, \\chi$ are hyperparameters and $f(\\chi, \\nu)$ depends on form of exponential family member\n",
        "\n",
        "We can verify that `posterior` and `prior` are indeed of same form\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(\\eta|x_{1:n},\\chi, \\nu) &\\propto p(x_{1:n}|\\eta)p(\\eta|\\chi, \\nu)\\\\\n",
        "&=\\left[\\left(\\prod_{i=1}^nh(x_i)\\right)g(\\eta)^n\\exp\\left(\\eta^T\\sum_{i=1}^nu(x_i)\\right)\\right]f(\\chi, \\nu)g(\\eta)^{\\nu}\\exp\\left[\\eta^T\\chi\\right]\\\\\n",
        "&=\\left(\\prod_{i=1}^nh(x_i)\\right)f(\\chi, \\nu)g(\\eta)^{n+\\nu}\\exp\\left(\\eta^T\\left(\\sum_{i=1}^nu(x_i)+\\chi\\right)\\right)\\\\\n",
        "&\\propto g(\\eta)^{n+\\nu}\\exp\\left(\\eta^T\\left(\\sum_{i=1}^nu(x_i)+\\chi\\right)\\right)\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "So for posterior\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\nu &= \\nu_{\\text{prior}}+n \\\\\n",
        "\\chi &= \\chi_{\\text{prior}}+\\sum_{i=1}^nu(x_i) \\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\nu_t &= \\left\\{\\begin{array}{ll}\\nu_{\\text{prior}} & t=0 \\\\ \\nu_{t-1}+1 & t>0\\end{array}\\right.\\ \\\\\n",
        "\\chi_t &= \\left\\{\\begin{array}{ll}\\chi_{\\text{prior}} & t=0 \\\\ \\chi_{t-1}+u(x_{t}) & t>0\\end{array}\\right.\\ \\\\\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "197W3EU2-9G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should be noted that neither natural parameter $\\eta$ nor hyperparameters $\\chi, \\nu$ are necessarily the ones we are familier with (mean, variance)\n",
        "\n",
        "When we translate back to the more familiar parameters, those same additions are still there, but they appear through whatever nonlinear transformation connects the two parameterizations"
      ],
      "metadata": {
        "id": "Ke2YghcbKSLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example of Gaussian"
      ],
      "metadata": {
        "id": "GwMqMFDzdqHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Gaussian as example, with `both` mean and variance as parameters\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x|\\mu, \\sigma^2)&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right] \\\\\n",
        "&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2\\sigma^2}x^2+\\frac{\\mu}{\\sigma^2}x-\\frac{\\mu^2}{2\\sigma^2}\\right]\\\\\n",
        "&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{\\mu^2}{2\\sigma^2}\\right]\\exp\\left[\\frac{\\mu}{\\sigma^2}x-\\frac{1}{2\\sigma^2}x^2\\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "By split like this, we can introduce sufficient statistic and natural parameter\n",
        "\n",
        "$$u(x)=\\begin{bmatrix}x \\\\ x^2\\end{bmatrix}, \\eta(\\mu, \\sigma^2)=\\begin{bmatrix}\\eta_1\\\\ \\eta_2\\end{bmatrix}=\\begin{bmatrix}\\frac{\\mu}{\\sigma^2}\\\\ -\\frac{1}{2\\sigma^2}\\end{bmatrix}$$\n",
        "\n",
        "In addition\n",
        "\n",
        "$$h(x)=\\frac{1}{\\sqrt{2\\pi}}, g(\\eta)=\\frac{1}{\\sigma}\\exp\\left[-\\frac{\\mu^2}{2\\sigma^2}\\right]$$\n",
        "\n",
        "Notice that\n",
        "\n",
        "$$\\frac{1}{\\sigma}=\\sqrt{-2\\eta_2}$$\n",
        "\n",
        "and\n",
        "\n",
        "$$-\\frac{\\mu^2}{2\\sigma^2}=\\frac{\\eta_1^2}{4\\eta_2}$$\n",
        "\n",
        "we can rewrite the normalizer as\n",
        "\n",
        "$$g(\\eta)=\\sqrt{-2\\eta_2}\\exp \\left[\\frac{\\eta_1^2}{4\\eta_2}\\right]$$"
      ],
      "metadata": {
        "id": "vSCqpPDNd4nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we `fix` the variance, we can split density as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x|\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2\\sigma^2}x^2\\right]\\exp\\left[-\\frac{\\mu^2}{2\\sigma^2}\\right]\\exp\\left[\\frac{\\mu}{\\sigma^2}x\\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\eta &= \\frac{\\mu}{\\sigma^2}\\\\\n",
        "u(x) & = x \\\\\n",
        "h(x) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2\\sigma^2}x^2\\right]\\\\\n",
        "g(\\eta) & =\\exp\\left[-\\frac{\\mu^2}{2\\sigma^2}\\right]=\\exp\\left[-\\frac{\\sigma^2\\eta^2}{2}\\right]\\\\\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "7bbQZCjaixNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `conjugate prior`\n",
        "\n",
        "$$p(\\eta|\\chi, \\nu)=f(\\chi, \\nu)g(\\eta)^{\\nu}\\exp\\left[\\eta^T\\chi\\right]$$\n",
        "\n",
        "In the case of fixed variance, we plug in $g(\\eta)$ and since $f(\\chi, \\nu)$ does not depend on $\\eta$, we have for natural parameter\n",
        "\n",
        "$$p(\\eta|\\chi, \\nu)\\propto \\exp \\left[-\\frac{\\nu\\sigma^2}{2}\\eta^2+\\chi\\eta\\right] \\sim N\\left(\\eta;\\frac{\\chi}{\\nu\\sigma^2},\\frac{1}{\\nu\\sigma^2}\\right)$$\n",
        "\n",
        "and for actual mean $\\mu=\\sigma^2\\eta$\n",
        "\n",
        "$$\\mu \\sim N\\left(\\mu; \\frac{\\chi}{\\nu}, \\frac{\\sigma^2}{\\nu}\\right)$$\n",
        "\n",
        "Write $\\mu_0=\\frac{\\chi}{\\nu}$ indicating the mean of prior (that is, taking zero observation into account)\n",
        "\n",
        "Same for $\\sigma_0^2=\\frac{\\sigma^2}{\\nu}$"
      ],
      "metadata": {
        "id": "Rd77GJM7ozl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `posterior`\n",
        "\n",
        "$$\\begin{align*}\n",
        "&p(\\eta|x_{1:n})\\sim N\\left(\\eta;\\frac{\\chi'}{\\nu'\\sigma^2}, \\frac{1}{\\nu'\\sigma^2}\\right) \\\\\n",
        "& \\text{since }\\mu=\\sigma^2\\eta,\n",
        "\\nu' = \\nu+n,\n",
        "\\chi' = \\chi+\\sum_{i=1}^nu(x_i) \\\\\n",
        "\\rightarrow & \\,p(\\mu|x_{1:n})\\sim N\\left(\\mu; \\mu_n, \\sigma_n^2\\right)=N\\left(\\mu;\\frac{\\chi+\\sum_{i=1}^nx_i}{\\nu+n},\\frac{\\sigma^2}{\\nu+n}\\right)\n",
        "\\end{align*}$$\n",
        "\n",
        "Plug in expression for common hyperparameter $\\mu_0$ and $\\sigma_0$, we have\n",
        "\n",
        "$$\\begin{align*}\n",
        "p(\\mu|x_{1:n})\\sim N\\left(\\mu;\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+n\\sigma_0^2},\\frac{\\sigma_0^2\\sigma^2}{\\sigma^2+n\\sigma_0^2}\\right)\n",
        "\\end{align*}$$\n",
        "\n",
        "We see it is the same as derived earlier using completion of square or optimization method, but without much calculation needed"
      ],
      "metadata": {
        "id": "V1MqPCASPQ8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Changepoint prior"
      ],
      "metadata": {
        "id": "NCSP9qN-dmHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute the changepoint prior $p(r_t|r_{t-1})$, we define the following functions\n",
        "\n",
        "  - $f(\\tau)$: prior probability that the current run will end right after the $\\tau$th data point. This is the prior knowledge before seeing any data\n",
        "  - $S(\\tau)$: survival function that current run is alive at least until the $\\tau$th data point has been obtained\n",
        "  - $H(\\tau)=f(\\tau)/S(\\tau)$: hazard function that `conditioned` on having survived through the $(\\tau-1)$th data point, the probability the current run `terminates` right after the $\\tau$th data point is observed\n",
        "\n",
        "More explicitly\n",
        "\n",
        "$$H(\\tau)=p(T=\\tau | T\\geq\\tau)=\\frac{p(T=\\tau)}{p(T\\geq \\tau)}=\\frac{f(\\tau)}{S(\\tau)}$$"
      ],
      "metadata": {
        "id": "gMebAB_5eEJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, the changepoint prior is a discrete probability distribution of two states\n",
        "\n",
        "$$p(r_t|r_{t-1})=\\left\\{\\begin{array}{ll}H(r_{t-1}+1) & r_t=0 \\\\ 1-H(r_{t-1}+1) & r_t=r_{t-1}+1\\end{array}\\right.$$\n",
        "\n",
        "That is, `before` we look at $x_t$, the model says “with prior probability $H(r_{t-1}+1)$ the current run will `end` right after this sample, otherwise it will continue”"
      ],
      "metadata": {
        "id": "eRLl3WK9gMZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQlZx64VFXL3"
      },
      "outputs": [],
      "source": []
    }
  ]
}