{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "RXTo9PJLGaxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `changepoint` is an underlying shift in the parameters that generate a data sequence (e.g. the mean of a Gaussian suddenly jumps). Here, we focus on the online or causal task\n",
        "\n",
        "To do this, we introduce a latent variable at time step $t$ named `run length` $r_t$, indicating the number of steps since the most recent changepoint\n",
        "\n",
        "Based on observation $x_t$, if a changepoint occurs at $t$, then $r_t=0$; otherwise it increments by 1 from $r_{t-1}$\n",
        "\n",
        "We want to maintains full run length `posterior` $p(r_t|x_{1:t})$ and update this `recursively`\n",
        "\n",
        "In addition, we want to update the sequence `predictive distribution` for $x_{t+1}$ using only $x_{1:t}$\n"
      ],
      "metadata": {
        "id": "RyYZGXpDFiF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predictive distribution"
      ],
      "metadata": {
        "id": "ym9Sr6iSG_Ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using marginalization, we can write the `predictive` as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x_{t+1}|x_{1:t})&=\\sum_{r_t=0}^tp(x_{t+1}|r_t, x_{t-r_t:t})p(r_t|x_{1:t}) \\\\\n",
        "&=\\sum_{r_t=0}^tp(x_{t+1}|r_t, x_t^{(r)})p(r_t|x_{1:t})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $x_t^{(r)}$ denotes the portion of $x_{1:t}$ that belongs to the `current` run\n",
        "\n",
        "This shows that sequence predictive is determined by $p(x_{t+1}|r_t, x_t^{(r)})$ and run length posterior $p(r_t|x_{1:t})$\n",
        "\n",
        "$p(x_{t+1}|r_t, x_t^{(r)})$ is often refered to as underlying probabilistic model `(UPM) predictive` to distinguish it from sequence predictive $p(x_{t+1}|x_{1:t})$"
      ],
      "metadata": {
        "id": "vN9ST-GGHCIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we look at run length posterior and UPM predictive"
      ],
      "metadata": {
        "id": "Vr0Uq-yo9Jdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run length posterior"
      ],
      "metadata": {
        "id": "zpxNKzVqJxxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute run length posterior, we use the expression of conditional probability\n",
        "\n",
        "$$\\begin{align*}\n",
        "p(r_t|x_{1:t})&=\\frac{p(r_t, x_{1:t})}{\\sum_{r_{t'}}p(r_{t'},x_{1:t})}\n",
        "\\end{align*}$$\n",
        "\n",
        "We express the joint in a `recursive` manner\n",
        "\n",
        "$$\\begin{align*}\n",
        "p(r_t, x_{1:t})&=\\sum_{r_{t-1}}p(r_t, r_{t-1}, x_t, x_{1:t-1}) \\\\\n",
        "&=\\sum_{r_{t-1}}p(r_t, x_t | r_{t-1},  x_{1:t-1})p(r_{t-1}, x_{1:t-1})\\\\\n",
        "&=\\sum_{r_{t-1}}p(x_t|r_t, r_{t-1}, x_{1:t-1})p(r_t|r_{t-1}, x_{1:t-1})p(r_{t-1}, x_{1:t-1}) \\\\\n",
        "& \\text{assumption: } x_t \\text{ conditionally independent of } r_t \\\\\n",
        "& \\text{assumption: } r_t \\text{ conditionally independent of } x_{1:t-1} \\\\\n",
        "&=\\sum_{r_{t-1}}p(x_t|r_{t-1}, x_{1:t-1})p(r_t|r_{t-1})p(r_{t-1}, x_{1:t-1})\\\\\n",
        "&=\\sum_{r_{t-1}}p(x_t|r_{t-1}, x_{t-1}^{(r)})p(r_t|r_{t-1})p(r_{t-1}, x_{1:t-1})\\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "Therefore, the joint at step $t$, $p(r_t, x_{1:t})$, depends on UPM predictive $p(x_t|r_{t-1}, x_{t-1}^{(r)})$ and joint $p(r_{t-1}, x_{1:t-1})$ at step $t-1$ plus a changepoint prior $p(r_t|r_{t-1})$"
      ],
      "metadata": {
        "id": "6_cUd86zJ1o1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is noted from derivation above that once we set the initial joint $p(r_0)$, what remains to do is to efficiently update UPM predictive and compute changepoint prior"
      ],
      "metadata": {
        "id": "wdtilTHGRt1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### UPM predictive"
      ],
      "metadata": {
        "id": "KgdBrxPE9ewf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The computation of UPM predictive leverages `conjugate model`"
      ],
      "metadata": {
        "id": "qboWzVjm9j8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Conjugate models"
      ],
      "metadata": {
        "id": "e0w_Ly1jS2TZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have observations $D$, model parameters $\\theta$ and hyperparameters $\\alpha$. Then the prior predictive distribution marginalized over parameters can be written as\n",
        "\n",
        "$$p(x|\\alpha)=\\int p(x|\\theta)p(\\theta |\\alpha)$$\n",
        "\n",
        "where $p(x|\\theta)$ is `predictive model` given parameters and $(\\theta |\\alpha)$ is the prior of `parameters`\n",
        "\n",
        "This is called `prior predictive distribution` because this is the prediction `before` we observe any data (that is, $D$ is not taken into account)"
      ],
      "metadata": {
        "id": "1s1ErMYWS3tW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can write `posterior predictive distribution` as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x|D, \\alpha)&=\\int p(x|\\theta)p(\\theta |D, \\alpha) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "A wonderful property of conjugate model is that the prior distribution and posterior distribution are of the same form. Therefore, if parameter prior is conjugate, then we have\n",
        "\n",
        "$$p(\\theta|D, \\alpha) = p(\\theta | \\alpha')$$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x|D, \\alpha)&=\\int p(x|\\theta)p(\\theta |D, \\alpha) \\\\\n",
        "&=\\int p(x|\\theta)p(\\theta |\\alpha')\\\\\n",
        "&=p(x|\\alpha')\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "That is, the posterior predictive distribution is in the `same form` as the prior predictive with only changed hyperparameters $\\alpha'$\n",
        "\n",
        "This allows us to bypass the whole integration thing provided that we can compute $\\alpha'$\n",
        "\n",
        "One family of conjugate model that is particularly attractive to efficiently compute $\\alpha'$ is the exponential family"
      ],
      "metadata": {
        "id": "AmSgonGpUI3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exponential family"
      ],
      "metadata": {
        "id": "x_P2FDM1YAhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard distributions with conjugate priors can be cast into a canonical form (we can think of it as `likelihood`)\n",
        "\n",
        "$$p(x|\\eta)=h(x)\\exp \\left[\\eta^T u(x)-A(\\eta)\\right]$$\n",
        "\n",
        "where $h(x)$ is base measure carrying every factor that does not involve $\\eta$, $u(x)$ is sufficient-statistics vector, $\\eta$ is `natural-parameter` vector, and $A(\\eta)$ is a normalizing function"
      ],
      "metadata": {
        "id": "6q_kqmn1bWqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `conjugate prior` for $\\eta$ is expressed as\n",
        "\n",
        "$$p(\\eta | \\chi, \\nu)=\\exp\\left[\\eta^T\\chi-\\nu A(\\eta)-\\tilde{A}(\\chi, \\nu)\\right]$$\n",
        "\n",
        "where $\\chi$ and $\\nu$ are `hyperparameters`"
      ],
      "metadata": {
        "id": "enCHQKrxb97Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compute its posterior given data $x_{1:n}$\n",
        "\n",
        "Let\n",
        "\n",
        "$$\\bar{u}=\\sum_{i=1}^n u(x_i)$$\n",
        "\n",
        "We then multiply likelihood with the prior\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(\\eta|x_{1:n}, \\chi, \\nu) &\\propto \\left[\\prod_{i=1}^np(x_i|\\eta)\\right]p(\\eta|\\chi, \\nu) \\\\\n",
        "& \\text{drop terms that are constant w.r.t. } \\eta \\\\\n",
        "&\\propto \\exp \\left[\\eta^T\\ (\\chi + \\bar{u})-(\\nu+n)A(\\eta)\\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "We can see that hyperparameter update is simply\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\nu'&=\\nu+n \\\\\n",
        "\\chi' &= \\chi + \\sum_{i=1}^nu(x_i)\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "2QuT-zFzclO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example of Gaussian"
      ],
      "metadata": {
        "id": "ocUJvY3L-Lwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, consider a Gaussian distribution with parameter $\\mu$ and `fixed` variance $\\sigma^2$, where $\\mu$ is determined by two hyperparameters $\\mu_0$ and $\\sigma_0^2$, or with `prior` $\\mu \\sim N(\\mu_0, \\sigma_0^2)$\n",
        "\n",
        "If we have $n$ data points $x_{1:n}$, we can write out the `likelihood`\n",
        "\n",
        "$$\n",
        "\\begin{align*}L(x_{1:n}|\\mu, \\sigma^2)&=\\prod_{i=1}^n p(x_i|\\mu) \\\\\n",
        "&=\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right]\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "MeeDaKjBFwJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now write the `joint`\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x_{1:n}, \\mu) &= \\frac{1}{\\sqrt{2\\pi \\sigma_0^2}}\\exp\\left[\\frac{1}{2\\sigma_0^2}(\\mu-\\mu_0)^2\\right]\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right]\\\\\n",
        "&\\propto \\exp\\left[-\\frac{1}{2\\sigma_0^2}\\left(\\mu^2-2\\mu \\mu_0+\\mu_0^2\\right)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\\left(x_i^2-2x_i\\mu+\\mu^2\\right)\\right]\\\\\n",
        "&=\\exp \\left[-\\frac{\\mu^2}{2\\sigma_0^2}+\\frac{\\mu\\mu_0}{\\sigma_0^2}-\\frac{\\mu_0^2}{2\\sigma_0^2}-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2 +\\frac{\\mu}{\\sigma^2}\\sum_{i=1}^nx_i -\\frac{\\mu^2}{2\\sigma^2}n\\right] \\\\\n",
        "&\\propto \\exp\\left[-\\frac{\\mu^2\\sigma^2+\\mu^2\\sigma_0^2n}{2\\sigma_0^2\\sigma^2}+\\frac{\\mu\\mu_0\\sigma^2+\\mu\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma_0^2\\sigma^2}\\right] \\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left((\\sigma^2+\\sigma_0^2n)\\mu^2-2(\\mu_0\\sigma^2+\\sigma_0^2\\sum_{i=1}^nx_i)\\mu\\right)\\right]\\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left(a\\mu^2+b\\mu+c\\right)\\right] \\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left(a(\\mu-d)^2+e\\right)\\right]\\\\\n",
        "a&=\\sigma^2+\\sigma_0^2n \\\\\n",
        "d&=-\\frac{b}{2a}=\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\\\\n",
        "e&=c-\\frac{b^2}{4a}=-\\frac{(\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i)^2}{\\sigma^2+\\sigma_0^2n} \\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left((\\sigma^2+\\sigma_0^2n)\\left(\\mu-\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\right)^2-\\frac{(\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i)^2}{\\sigma^2+\\sigma_0^2n}\\right)\\right] \\\\\n",
        "&\\propto \\exp\\left[-\\frac{\\sigma^2+\\sigma_0^2n}{2\\sigma_0^2\\sigma^2}\\left(\\mu-\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\right)^2\\right]\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "MXqoLiAMMcAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Bayes' rule, we know that `posterior` is proportional to this joint and therefore\n",
        "\n",
        "$$p(\\mu|x_{1:n})\\propto \\exp\\left[-\\frac{\\sigma^2+\\sigma_0^2n}{2\\sigma_0^2\\sigma^2}\\left(\\mu-\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\right)^2\\right]$$\n",
        "\n",
        "Therefore, the posterior `mean` is\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mu' &= \\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n} \\\\\n",
        "&=\\frac{\\sigma^2}{\\sigma^2+\\sigma_0^2n}\\mu_0+\\frac{\\sigma_0^2}{\\sigma^2+\\sigma_0^2n}\\sum_{i=1}^nx_i\n",
        "\\end{align*}$$\n",
        "\n",
        "Recall that $\\frac{1}{N}\\sum_{i=1}^nx_i$ is the `maximum likelihood` estimate of $\\mu$\n",
        "\n",
        "So the posterior mean is a weighted sum of prior mean $\\mu_0$ and the ML estimate $\\mu_{ML}$\n",
        "\n",
        "The posterior `variance` is\n",
        "\n",
        "$$\\sigma'^2 = \\left[\\frac{\\sigma_0^2\\sigma^2}{\\sigma^2+\\sigma_0^2}\\right]^2$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{1}{\\sigma'} &=\\frac{\\sigma^2+\\sigma_0^2n}{\\sigma_0^2\\sigma^2} \\\\\n",
        "&=\\frac{\\sigma^2}{\\sigma_0^2\\sigma^2}+\\frac{\\sigma_0^2}{\\sigma^2\\sigma_0^2}n \\\\\n",
        "&=\\frac{1}{\\sigma_0^2}+\\frac{1}{\\sigma^2}n\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "r19srax5RWtb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQlZx64VFXL3"
      },
      "outputs": [],
      "source": []
    }
  ]
}