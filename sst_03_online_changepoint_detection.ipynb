{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "RXTo9PJLGaxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `changepoint` is an underlying shift in the parameters that generate a data sequence (e.g. the mean of a Gaussian suddenly jumps). Here, we focus on the online or causal task\n",
        "\n",
        "To do this, we introduce a latent variable at time step $t$ named `run length` $r_t$, indicating the number of steps since the most recent changepoint\n",
        "\n",
        "Based on observation $x_t$, if a changepoint occurs at $t$, then $r_t=0$; otherwise it increments by 1 from $r_{t-1}$\n",
        "\n",
        "We want to maintains full run length `posterior` $p(r_t|x_{1:t})$ and update this `recursively`\n",
        "\n",
        "In addition, we want to update the sequence `predictive distribution` for $x_{t+1}$ using only $x_{1:t}$\n"
      ],
      "metadata": {
        "id": "RyYZGXpDFiF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predictive distribution"
      ],
      "metadata": {
        "id": "ym9Sr6iSG_Ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using marginalization, we can write the `predictive` as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x_{t+1}|x_{1:t})&=\\sum_{r_t=0}^tp(x_{t+1}|r_t, x_{t-r_t:t})p(r_t|x_{1:t}) \\\\\n",
        "&=\\sum_{r_t=0}^tp(x_{t+1}|r_t, x_t^{(r)})p(r_t|x_{1:t})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $x_t^{(r)}$ denotes the portion of $x_{1:t}$ that belongs to the `current` run\n",
        "\n",
        "This shows that sequence predictive is determined by $p(x_{t+1}|r_t, x_t^{(r)})$ and run length posterior $p(r_t|x_{1:t})$\n",
        "\n",
        "$p(x_{t+1}|r_t, x_t^{(r)})$ is often refered to as underlying probabilistic model `(UPM) predictive` to distinguish it from sequence predictive $p(x_{t+1}|x_{1:t})$"
      ],
      "metadata": {
        "id": "vN9ST-GGHCIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we look at run length posterior and UPM predictive"
      ],
      "metadata": {
        "id": "Vr0Uq-yo9Jdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run length posterior"
      ],
      "metadata": {
        "id": "zpxNKzVqJxxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute run length posterior, we use the expression of conditional probability\n",
        "\n",
        "$$\\begin{align*}\n",
        "p(r_t|x_{1:t})&=\\frac{p(r_t, x_{1:t})}{\\sum_{r_{t'}}p(r_{t'},x_{1:t})}\n",
        "\\end{align*}$$\n",
        "\n",
        "We express the joint in a `recursive` manner\n",
        "\n",
        "$$\\begin{align*}\n",
        "p(r_t, x_{1:t})&=\\sum_{r_{t-1}}p(r_t, r_{t-1}, x_t, x_{1:t-1}) \\\\\n",
        "&=\\sum_{r_{t-1}}p(r_t, x_t | r_{t-1},  x_{1:t-1})p(r_{t-1}, x_{1:t-1})\\\\\n",
        "&=\\sum_{r_{t-1}}p(x_t|r_t, r_{t-1}, x_{1:t-1})p(r_t|r_{t-1}, x_{1:t-1})p(r_{t-1}, x_{1:t-1}) \\\\\n",
        "& \\text{assumption: } x_t \\text{ conditionally independent of } r_t \\\\\n",
        "& \\text{assumption: } r_t \\text{ conditionally independent of } x_{1:t-1} \\\\\n",
        "&=\\sum_{r_{t-1}}p(x_t|r_{t-1}, x_{1:t-1})p(r_t|r_{t-1})p(r_{t-1}, x_{1:t-1})\\\\\n",
        "&=\\sum_{r_{t-1}}p(x_t|r_{t-1}, x_{t-1}^{(r)})p(r_t|r_{t-1})p(r_{t-1}, x_{1:t-1})\\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "Therefore, the joint at step $t$, $p(r_t, x_{1:t})$, depends on UPM predictive $p(x_t|r_{t-1}, x_{t-1}^{(r)})$ and joint $p(r_{t-1}, x_{1:t-1})$ at step $t-1$ plus a changepoint prior $p(r_t|r_{t-1})$"
      ],
      "metadata": {
        "id": "6_cUd86zJ1o1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is noted from derivation above that once we set the initial joint $p(r_0)$, what remains to do is to efficiently update UPM predictive and compute changepoint prior"
      ],
      "metadata": {
        "id": "wdtilTHGRt1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conjugacy"
      ],
      "metadata": {
        "id": "e0w_Ly1jS2TZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The computation of UPM predictive leverages `conjugate model`"
      ],
      "metadata": {
        "id": "qboWzVjm9j8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have observations $D$, model parameters $\\theta$ and hyperparameters $\\alpha$. Then the prior predictive distribution marginalized over parameters can be written as\n",
        "\n",
        "$$p(x|\\alpha)=\\int p(x|\\theta)p(\\theta |\\alpha)d\\theta$$\n",
        "\n",
        "where $p(x|\\theta)$ is `predictive model` given parameters and $(\\theta |\\alpha)$ is the prior of `parameters`\n",
        "\n",
        "This is called `prior predictive distribution` because this is the prediction `before` we observe any data (that is, $D$ is not taken into account)"
      ],
      "metadata": {
        "id": "1s1ErMYWS3tW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can write `posterior predictive distribution` as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x|D, \\alpha)&=\\int p(x|\\theta)p(\\theta |D, \\alpha)d\\theta \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "A wonderful property of `conjugate` model is that the prior distribution and posterior distribution are of the same form. Therefore, if the prior and posterior of parameter are conjugate w.r.t. certain likelihood function, then we have\n",
        "\n",
        "$$p(\\theta|D, \\alpha) = p(\\theta | \\alpha')$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x|D, \\alpha)&=\\int p(x|\\theta)p(\\theta |D, \\alpha)d\\theta \\\\\n",
        "&=\\int p(x|\\theta)p(\\theta |\\alpha')d\\theta\\\\\n",
        "&=p(x|\\alpha')\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "That is, the posterior predictive distribution is in the `same form` as the prior predictive with only changed hyperparameters $\\alpha'$\n",
        "\n",
        "This allows us to bypass the whole integration thing provided that we can compute $\\alpha'$\n",
        "\n",
        "One family of conjugate model that is particularly attractive to efficiently compute $\\alpha'$ is the exponential family"
      ],
      "metadata": {
        "id": "AmSgonGpUI3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example of Gaussian"
      ],
      "metadata": {
        "id": "ocUJvY3L-Lwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, consider a Gaussian distribution with parameter $\\mu$ and `fixed` variance $\\sigma^2$, where $\\mu$ is determined by two hyperparameters $\\mu_0$ and $\\sigma_0^2$, or with `prior` $\\mu \\sim N(\\mu_0, \\sigma_0^2)$\n",
        "\n",
        "If we have $n$ data points $x_{1:n}$, we can write out the `likelihood`\n",
        "\n",
        "$$\n",
        "\\begin{align*}L(x_{1:n}|\\mu, \\sigma^2)&=\\prod_{i=1}^n p(x_i|\\mu) \\\\\n",
        "&=\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right]\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "MeeDaKjBFwJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now write the `joint`\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x_{1:n}, \\mu) &= \\frac{1}{\\sqrt{2\\pi \\sigma_0^2}}\\exp\\left[-\\frac{1}{2\\sigma_0^2}(\\mu-\\mu_0)^2\\right]\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right]\\\\\n",
        "&\\propto \\exp\\left[-\\frac{1}{2\\sigma_0^2}\\left(\\mu^2-2\\mu \\mu_0+\\mu_0^2\\right)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\\left(x_i^2-2x_i\\mu+\\mu^2\\right)\\right]\\\\\n",
        "&=\\exp \\left[-\\frac{\\mu^2}{2\\sigma_0^2}+\\frac{\\mu\\mu_0}{\\sigma_0^2}-\\frac{\\mu_0^2}{2\\sigma_0^2}-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2 +\\frac{\\mu}{\\sigma^2}\\sum_{i=1}^nx_i -\\frac{\\mu^2}{2\\sigma^2}n\\right] \\\\\n",
        "&\\propto \\exp\\left[-\\frac{\\mu^2\\sigma^2+\\mu^2\\sigma_0^2n}{2\\sigma_0^2\\sigma^2}+\\frac{\\mu\\mu_0\\sigma^2+\\mu\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma_0^2\\sigma^2}\\right] \\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left((\\sigma^2+\\sigma_0^2n)\\mu^2-2(\\mu_0\\sigma^2+\\sigma_0^2\\sum_{i=1}^nx_i)\\mu\\right)\\right]\\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left(a\\mu^2+b\\mu+c\\right)\\right] \\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left(a(\\mu-d)^2+e\\right)\\right]\\\\\n",
        "a&=\\sigma^2+\\sigma_0^2n \\\\\n",
        "d&=-\\frac{b}{2a}=\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\\\\n",
        "e&=c-\\frac{b^2}{4a}=-\\frac{(\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i)^2}{\\sigma^2+\\sigma_0^2n} \\\\\n",
        "&=\\exp\\left[-\\frac{1}{2\\sigma_0^2\\sigma^2}\\left((\\sigma^2+\\sigma_0^2n)\\left(\\mu-\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\right)^2-\\frac{(\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i)^2}{\\sigma^2+\\sigma_0^2n}\\right)\\right] \\\\\n",
        "&\\propto \\exp\\left[-\\frac{\\sigma^2+\\sigma_0^2n}{2\\sigma_0^2\\sigma^2}\\left(\\mu-\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\right)^2\\right]\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "MXqoLiAMMcAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Bayes' rule, we know that `posterior` is proportional to this joint and therefore\n",
        "\n",
        "$$p(\\mu|x_{1:n})\\propto \\exp\\left[-\\frac{\\sigma^2+\\sigma_0^2n}{2\\sigma_0^2\\sigma^2}\\left(\\mu-\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n}\\right)^2\\right]$$\n",
        "\n",
        "which is a Gaussian distribution as well"
      ],
      "metadata": {
        "id": "r19srax5RWtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, the posterior `variance` is\n",
        "\n",
        "$$\\sigma_n^2 = \\left[\\frac{\\sigma_0^2\\sigma^2}{\\sigma^2+\\sigma_0^2}\\right]^2$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{1}{\\sigma_n^2} &=\\frac{\\sigma^2+\\sigma_0^2n}{\\sigma_0^2\\sigma^2} \\\\\n",
        "&=\\frac{\\sigma^2}{\\sigma_0^2\\sigma^2}+\\frac{\\sigma_0^2}{\\sigma^2\\sigma_0^2}n \\\\\n",
        "&=\\frac{1}{\\sigma_0^2}+\\frac{1}{\\sigma^2}n\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\sigma_n^2 =\\frac{\\sigma_0^2\\sigma^2}{\\sigma^2+\\sigma_0^2n}= \\frac{1}{\\frac{1}{\\sigma_0^2}+\\frac{1}{\\sigma^2}n}$$"
      ],
      "metadata": {
        "id": "eYb3Mv9UngGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The posterior `mean` is\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mu_n &= \\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+\\sigma_0^2n} \\\\\n",
        "&=\\frac{\\sigma^2}{\\sigma^2+\\sigma_0^2n}\\mu_0+\\frac{\\sigma_0^2}{\\sigma^2+\\sigma_0^2n}\\sum_{i=1}^nx_i \\\\\n",
        "&=\\sigma_n^2\\left(\\frac{\\mu_0}{\\sigma_0^2}+\\frac{1}{\\sigma^2}\\sum_{i=1}^nx_i\\right)\n",
        "\\end{align*}$$\n",
        "\n",
        "Recall that $\\frac{1}{n}\\sum_{i=1}^nx_i$ is the `maximum likelihood` estimate of $\\mu$\n",
        "\n",
        "So the posterior mean is a weighted sum of prior mean $\\mu_0$ and the ML estimate $\\mu_{ML}$\n"
      ],
      "metadata": {
        "id": "bt1J4AK_nijW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Derivation from perspective of optimization"
      ],
      "metadata": {
        "id": "Z7OV-olZsYSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the posterior $p(\\mu| x_{1:n})$ is Gaussian, therefore, the center of posterior density is also the maximum-a-posteriori (MAP) estimate of the mean $\\mu_n=\\mu_{\\text{MAP}}$\n",
        "\n",
        "If we just want to get this MAP estimate, we can take derivative of $\\log p(\\mu|x_{1:n})$ or for calculation, the log of joint $\\log p(x_{1:n}, \\mu)$ (since the difference in constant vanishes after differentiation)\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{d}{d\\mu}\\log p(x_{1:n}, \\mu)&=\\frac{d}{d\\mu}\\left[-\\frac{1}{2\\sigma_0^2}(\\mu-\\mu_0)^2-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2+\\text{const.}\\right] \\\\\n",
        "&=\\frac{d}{d\\mu}\\left[-\\frac{1}{2\\sigma_0^2}(\\mu^2-2\\mu\\mu_0)-\\frac{1}{2\\sigma^2}\\left(n\\mu^2-2\\mu\\sum_{i=1}^nx_i\\right)+\\text{const.}\\right] \\\\\n",
        "& \\text{group } \\mu^2 \\text{ and } \\mu \\\\\n",
        "&=\\frac{d}{d\\mu}\\left[-\\frac{\\sigma^2+n\\sigma_0^2}{2\\sigma_0^2\\sigma^2}\\mu^2+\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma_0^2\\sigma^2}\\mu +\\text{const.}\\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "and set to zero, we have\n",
        "\n",
        "$$\\frac{\\sigma^2+n\\sigma_0^2}{\\sigma_0^2\\sigma^2}\\mu=\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma_0^2\\sigma^2} $$\n",
        "\n",
        "Solve for the MAP estimate\n",
        "\n",
        "$$\\mu_{\\text{MAP}}=\\frac{\\sigma^2\\mu_0+\\sigma_0^2\\sum_{i=1}^nx_i}{\\sigma^2+n\\sigma_0^2}$$\n",
        "\n",
        "which is the same as the posterior mean $\\mu_n$ we get earlier\n",
        "\n",
        "Both approaches, taking the derivative or completing the square, are mathematically identical\n",
        "\n",
        "The latter just skips writing the derivative because the result can be read off immediately once the quadratic is centered"
      ],
      "metadata": {
        "id": "qKofWDrdsao5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same can be done for posterior variance\n",
        "\n",
        "Since the posterior is Gaussian, we know\n",
        "\n",
        "$$\\log p(\\mu|x_{1:n})=-\\frac{1}{2\\sigma_n^2}(\\mu-\\mu_n)^2+\\text{const.}$$\n",
        "\n",
        "Take second derivative w.r.t. $\\mu$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\left.\\frac{d^2}{d\\mu^2}\\log p(\\mu|x_{1:n})\\right|_{\\mu=\\mu_n}\n",
        "&=\\left.\\frac{d^2}{d\\mu^2}\\log p(\\mu,x_{1:n})\\right|_{\\mu=\\mu_n}\\\\\n",
        "&=-\\frac{1}{\\sigma_n^2} \\\\\n",
        "&=-\\frac{\\sigma^2+n\\sigma_0^2}{\\sigma_0^2\\sigma^2}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "and we get posterior variance as before\n",
        "\n",
        "$$\\sigma_n^2 =\\frac{\\sigma_0^2\\sigma^2}{\\sigma^2+n\\sigma_0^2}$$"
      ],
      "metadata": {
        "id": "3PYvFqKOyCZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exponential family"
      ],
      "metadata": {
        "id": "x_P2FDM1YAhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For exponential family, standard distributions with conjugate priors can be cast into a canonical form (we can think of it as `likelihood`)\n",
        "\n",
        "$$p(x|\\eta)=h(x)g(\\eta)\\exp \\left[\\eta^T u(x)\\right]$$\n",
        "\n",
        "where\n",
        "  - $h(x)$ is `underlying measure` carrying every factor that does not involve $\\eta$\n",
        "  - $u(x)$ is sufficient-statistic of `data`\n",
        "  - $\\eta$ is `natural-parameter`\n",
        "  - $g(\\eta)$ is normalizer\n",
        "\n",
        "$$\\begin{align*}\n",
        "g(\\eta) \\int h(x) \\exp\\left[\\eta^T u(x)\\right]dx=1\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "6q_kqmn1bWqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Sufficient statistic"
      ],
      "metadata": {
        "id": "-IjrVAHn1iXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For data $x_{1:n}$, the idea of sufficient statistic is that we can use $\\sum u(x_{i:n)$ to compute maximum likelihood eastimates of natural parameters\n",
        "\n",
        "Use canonical form, we can write the log-likelihood as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\log p(x_{1:n}|\\eta)&=\\log \\prod_{i=1}^n p(x_i|\\eta)\\\\\n",
        "&=\\log \\left[\\left(\\prod_{i=1}^nh(x_i)\\right)g(\\eta)^n\\exp\\left(\\eta^T\\sum_{i=1}^nu(x_i)\\right)\\right]\\\\\n",
        "&=\\log \\left(\\prod_{i=1}^nh(x_i)\\right)+n\\log g(\\eta)+\\eta^T\\sum_{i=1}^nu(x_i)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Take gradient w.r.t. $\\eta$ and set to zero\n",
        "\n",
        "$$\\nabla n\\log g(\\eta_{\\text{ML}})+\\sum_{i=1}^nu(x_i)=0$$\n",
        "\n",
        "or\n",
        "\n",
        "$$-\\nabla \\log g(\\eta_{\\text{ML}}) = \\frac{1}{n}\\sum_{i=1}^nu(x_i)$$\n",
        "\n",
        "Note that since sufficient statistic is a running sum, we can compute it `incrementally` as new data arrives"
      ],
      "metadata": {
        "id": "En2w47YY1ldG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Moments through differentiation"
      ],
      "metadata": {
        "id": "N29aN44R-GQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Denote log normalizer\n",
        "\n",
        "$$A(\\eta)=-\\log g(\\eta)=\\log \\int h(x)\\exp \\left[\\eta^Tu(x)\\right]dx$$\n",
        "\n",
        "Take gradient w.r.t. $\\eta$\n",
        "\n",
        "$$\\nabla A(\\eta)=\\frac{1}{Z(\\eta)}\\nabla Z(\\eta)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$Z(\\eta)=\\exp[A(\\eta)]=\\int h(x) \\exp\\left[\\eta^Tu(x)\\right]dx$$\n",
        "\n",
        "Under some mild condition, we can write\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla Z(\\eta)&=\\int h(x)u(x)\\exp\\left[\\eta^Tu(x)\\right]dx \\\\\n",
        "&=\\int u(x)Z(\\eta)\\frac{h(x)\\exp\\left[\\eta^Tu(x)\\right]}{Z(\\eta)}dx\\\\\n",
        "& \\text{the fraction is a density on }x\\\\\n",
        "&=\\mathbb{E}\\left[Z(\\eta)u(x)\\right]\\\\\n",
        "&=Z(\\eta)\\mathbb{E}\\left[u(x)\\right]\n",
        "\\end{align*}$$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$$\\nabla A(\\eta)=\\frac{1}{Z(\\eta)}\\nabla Z(\\eta)=\\mathbb{E}\\left[u(x)\\right]$$"
      ],
      "metadata": {
        "id": "sG4XvVXW-N1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example of Gaussian"
      ],
      "metadata": {
        "id": "GwMqMFDzdqHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, using Gaussian as example, with both mean and variance as parameters\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(x|\\mu, \\sigma^2)&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right] \\\\\n",
        "&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2\\sigma^2}x^2+\\frac{\\mu}{\\sigma^2}x-\\frac{\\mu^2}{2\\sigma^2}\\right]\\\\\n",
        "&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{\\mu^2}{2\\sigma^2}\\right]\\exp\\left[\\frac{\\mu}{\\sigma^2}-\\frac{1}{2\\sigma^2}x^2\\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "By split like this, we can introduce sufficient statistic and natural parameter\n",
        "\n",
        "$$u(x)=\\begin{bmatrix}x \\\\ x^2\\end{bmatrix}, \\eta(\\mu, \\sigma^2)=\\begin{bmatrix}\\eta_1\\\\ \\eta_2\\end{bmatrix}=\\begin{bmatrix}\\frac{\\mu}{\\sigma^2}\\\\ -\\frac{1}{2\\sigma^2}\\end{bmatrix}$$\n",
        "\n",
        "In addition\n",
        "\n",
        "$$h(x)=\\frac{1}{\\sqrt{2\\pi}}, g(\\eta)=\\frac{1}{\\sigma}\\exp\\left[-\\frac{\\mu^2}{2\\sigma^2}\\right]$$\n",
        "\n",
        "Notice that\n",
        "\n",
        "$$\\frac{1}{\\sigma}=\\sqrt{-2\\eta_2}$$\n",
        "\n",
        "and\n",
        "\n",
        "$$-\\frac{\\mu^2}{2\\sigma^2}=\\frac{\\eta_1^2}{4\\eta_2}$$\n",
        "\n",
        "we can rewrite the normalizer as\n",
        "\n",
        "$$g(\\eta)=\\sqrt{-2\\eta_2}\\frac{\\eta_1^2}{4\\eta_2}$$"
      ],
      "metadata": {
        "id": "vSCqpPDNd4nT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQlZx64VFXL3"
      },
      "outputs": [],
      "source": []
    }
  ]
}